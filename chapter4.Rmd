# IODS week4 RStudio exercise

Sina Hulkkonen
20/11/2018

*Describe the work you have done this week and summarize your learning.*

I did all the DataCamp exercises for this week. This week's exercises reminded me of factor analysis which I've done for one article, and I'm getting exited as I realized I might have learnt something new!

#2. Data Boston
```{r}
# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)
summary(Boston)
```
Boston is a dataset with 506 observations in 14 variables. It describes the city of Boston, US. There's information on the city's economics, pollution and crime rates, for example. The variables are listed below:
< code >crim
per capita crime rate by town.

< code >zn
proportion of residential land zoned for lots over 25,000 sq.ft.

< code >indus
proportion of non-retail business acres per town.

< code >chas
Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

< code >nox
nitrogen oxides concentration (parts per 10 million).

< code >rm
average number of rooms per dwelling.

< code >age
proportion of owner-occupied units built prior to 1940.

< code >dis
weighted mean of distances to five Boston employment centres.

< code >rad
index of accessibility to radial highways.

< code >tax
full-value property-tax rate per \$10,000.

< code >ptratio
pupil-teacher ratio by town.

< code >black
1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.

< code >lstat
lower status of the population (percent).

< code >medv
median value of owner-occupied homes in \$1000s.

#3. Graphical overwiew and correlations
```{r}
library(tidyr)
library(corrplot)
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

```
The dots represents correlations. The more intense color and the bigger in the dot, the stronger the correlation. Blue means positive and red negative correlation. For example, there is a strong correlation between rad and tax, meaning index of accessibility to radial highways is bigger (easier?) when full-value property-tax rate per \$10,000 is greater.

#4. Scaling and dividing the data
```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled<-as.data.frame(boston_scaled)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels=c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]
```

#5. Fittind LDA model on the train data
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes,pch=classes)
lda.arrows(lda.fit, myscale = 10)
```
It looks here, that LD1 explains 94.8% of the variance of the crime classes.

#6. Deleting the true values of "crim" and predicting them with LDA model to test data
```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```
It looks that the LDA model didn't predict everything correctly, but was better with "high"" crime rate quintile than the other quintiles. The model predicted 26 cases of "high" (25 correct). But with quintile "low" it had more error, as it predicted 20 out of (20+11=) 31 correct cases. Also, it classified 13 out of (1+13+4=) 18 "med low" and 15 out of (12+15+1=) 28 cases of "med high" correctly.

#7. K-means
```{r}
# access the MASS package
library(MASS)

# load the data
data("Boston")

# explore the dataset
str(Boston)
summary(Boston)

#scale
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled<-as.data.frame(boston_scaled)

#calculating the distances
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

#searching for the right number of clusters
set.seed(123)

k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

library(ggplot2)

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```
As wee see in gplot, the line drops most radically, when the number of clusters is 2. In pairs plot, the observations are shown in red or black belonging to each clusted as defined by the variables in the plot.

I didn't do the bonus exercises.